{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWzYuLoIsVbs",
        "outputId": "62d65373-6260-46e1-d4c1-27cc239cce24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/lite/python/util.py:55: DeprecationWarning: jax.xla_computation is deprecated. Please use the AOT APIs; see https://jax.readthedocs.io/en/latest/aot.html. For example, replace xla_computation(f)(*xs) with jit(f).lower(*xs).compiler_ir('hlo'). See CHANGELOG.md for 0.4.30 for more examples.\n",
            "  from jax import xla_computation as _xla_computation\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import gym\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "num_inputs = 4\n",
        "num_actions = 1  # Deterministic actions\n",
        "num_hidden = 128\n",
        "gamma = 0.99\n",
        "exploration_noise_std = 0.1\n",
        "max_steps_per_episode = 500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g-yOknWsbaP",
        "outputId": "ccd834ee-7ebd-4623-b929-4d973e0657a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Actor model\n",
        "actor_inputs = layers.Input(shape=(num_inputs,))\n",
        "actor_hidden = layers.Dense(num_hidden, activation=\"relu\")(actor_inputs)\n",
        "actor_output = layers.Dense(num_actions, activation=\"tanh\")(actor_hidden)\n",
        "actor_model = tf.keras.Model(inputs=actor_inputs, outputs=actor_output)"
      ],
      "metadata": {
        "id": "WUOuMdRJsdje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Critic model\n",
        "critic_inputs = layers.Input(shape=(num_inputs + num_actions,))\n",
        "critic_hidden = layers.Dense(num_hidden, activation=\"relu\")(critic_inputs)\n",
        "q_value = layers.Dense(1)(critic_hidden)\n",
        "critic_model = tf.keras.Model(inputs=critic_inputs, outputs=q_value)\n",
        "\n",
        "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "rYTYB1t5shOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizers\n",
        "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "\n",
        "# Replay buffer setup\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        self.batch_size = batch_size\n",
        "        self.buffer_counter = 0\n",
        "        self.state_buffer = np.zeros((buffer_capacity, num_inputs))\n",
        "        self.action_buffer = np.zeros((buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((buffer_capacity, num_inputs))\n",
        "        self.done_buffer = np.zeros((buffer_capacity, 1))\n",
        "\n",
        "    def store(self, state, action, reward, next_state, done):\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "        self.state_buffer[index] = state\n",
        "        self.action_buffer[index] = action\n",
        "        self.reward_buffer[index] = reward\n",
        "        self.next_state_buffer[index] = next_state\n",
        "        self.done_buffer[index] = done\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    def sample(self):\n",
        "        max_buffer = min(self.buffer_counter, self.buffer_capacity)\n",
        "        batch_indices = np.random.choice(max_buffer, self.batch_size)\n",
        "        return (\n",
        "            self.state_buffer[batch_indices],\n",
        "            self.action_buffer[batch_indices],\n",
        "            self.reward_buffer[batch_indices],\n",
        "            self.next_state_buffer[batch_indices],\n",
        "            self.done_buffer[batch_indices],\n",
        "        )\n",
        "\n",
        "# Initialize replay buffer\n",
        "buffer = ReplayBuffer()"
      ],
      "metadata": {
        "id": "Tuy_l8OhsjLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "for episode in range(1000):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "\n",
        "    for step in range(max_steps_per_episode):\n",
        "        state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
        "        state_tensor = tf.expand_dims(state_tensor, axis=0)\n",
        "\n",
        "        # Actor chooses action deterministically\n",
        "        action = actor_model(state_tensor).numpy()[0]\n",
        "        action += np.random.normal(0, exploration_noise_std)  # Add noise for exploration\n",
        "        action = np.clip(action, -1, 1)  # Ensure action is within valid range\n",
        "\n",
        "        # Execute action in environment\n",
        "        next_state, reward, done, _ = env.step(int(action > 0))  # Convert continuous action to discrete\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Store transition in replay buffer\n",
        "        buffer.store(state, action, reward, next_state, done)\n",
        "\n",
        "        # Train models if the replay buffer has enough samples\n",
        "        if buffer.buffer_counter >= buffer.batch_size:\n",
        "            states, actions, rewards, next_states, dones = buffer.sample()\n",
        "\n",
        "            # Convert to tensors\n",
        "            states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
        "            actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
        "            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
        "            dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
        "\n",
        "            # Critic training\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Target Q-value\n",
        "                target_actions = actor_model(next_states)\n",
        "                target_q_values = rewards + (1 - dones) * gamma * critic_model(\n",
        "                    tf.concat([next_states, target_actions], axis=1)\n",
        "                )\n",
        "                # Predicted Q-value\n",
        "                predicted_q_values = critic_model(tf.concat([states, actions], axis=1))\n",
        "                critic_loss = tf.keras.losses.MSE(target_q_values, predicted_q_values)\n",
        "\n",
        "            critic_grads = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "            critic_optimizer.apply_gradients(zip(critic_grads, critic_model.trainable_variables))\n",
        "\n",
        "            # Actor training\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Actor loss is the negative of the Q-value\n",
        "                actions_pred = actor_model(states)\n",
        "                actor_loss = -tf.reduce_mean(critic_model(tf.concat([states, actions_pred], axis=1)))\n",
        "\n",
        "            actor_grads = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "            actor_optimizer.apply_gradients(zip(actor_grads, actor_model.trainable_variables))\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "    print(f\"Episode {episode + 1}: Reward: {episode_reward:.2f}\")\n",
        "\n",
        "    if episode_reward > 475:\n",
        "        print(\"Solved!\")\n",
        "        break"
      ],
      "metadata": {
        "id": "SMzQPfgQ4Tbf",
        "outputId": "d4ec1a1e-3ccc-44f8-82bf-6ddb4f4c66ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-429cab3f941e>:16: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  next_state, reward, done, _ = env.step(int(action > 0))  # Convert continuous action to discrete\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1: Reward: 25.00\n",
            "Episode 2: Reward: 11.00\n",
            "Episode 3: Reward: 13.00\n"
          ]
        }
      ]
    }
  ]
}